# Day 12

## Most Important Libraries for Data Collection 

### 1. Scrapy

Every data scientistâ€™s project begins with data, and the Internet is the largest, richest, and most accessible trove of data. Unfortunately, beyond `pd.read_html`, most data scientists are clueless when it comes to scraping data off websites that have complex data structures. Scrapy makes building web crawlers that analyze the anatomy of a website and store the extracted information much easier than building it from scratch would be.

Here is the [link](https://docs.scrapy.org/en/latest/index.html "link") for official documentation


### 2. Pattern

Some websites that are more well-established may already have a more concrete method of retrieving data, and in this case using Scrapy to write a web crawler could be overkill. Pattern is a more high-level web mining and natural language processing module in Python.
Not only does it have seamless integration with Google, Twitter, and Wikipedia data, as well as a less customizable web crawler and an HTML DOM parser, Pattern employs POS (part of speech) tagging, n-grams searching, sentiment analysis, and WordNet. The result of preprocessed text data can be used in a variety of implemented machine learning algorithms, from clustering to classification, or visualized with network analysis.
Pattern has everything in the data science pipeline, from data retrieval to preprocessing to modelling and visualization, and avoids clumsy transferring of data across a mix of different libraries.

Here is the [link](https://github.com/clips/pattern "link") for official Github Repo
